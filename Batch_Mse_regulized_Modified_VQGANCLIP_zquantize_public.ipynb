{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Batch_Mse_regulized_Modified_VQGANCLIP_zquantize_public.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Midgraph/VQGAN-CLIP/blob/main/Batch_Mse_regulized_Modified_VQGANCLIP_zquantize_public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CppIQlPhhwhs"
      },
      "source": [
        "# Generates images from text prompts with VQGAN and CLIP (Mse regulized zquantize method).\n",
        "\n",
        "By jbustter https://twitter.com/jbusted1 .\n",
        "Based on a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n",
        "\n",
        "\n",
        "*Modified by: Justin John*\n",
        "\n",
        "\n",
        "Expanded with code from Danny Perry's [batch VQGAN processor](https://colab.research.google.com/drive/1sfZIuv8d40dXlPaNBpXgHMwLZ3VkLgnG?usp=sharing#scrollTo=eD4YyZDessWK)  notebook: [@Datamosh](https://instagram.com/datamosh) and [artificial_art_](https://twitter.com/artificial_art_)\n",
        "\n",
        "\n",
        "*by Max Ingham of [Midgraph](www.midgraph.com) and [@somnai_dreams](https://twitter.com/somnai_dreams)*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCuopwEiNdAR",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82bf856-fd09-4274-c1ec-066a59403e08"
      },
      "source": [
        "#@markdown #**Check GPU type**\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro Users*)\n",
        "\n",
        "#@markdown P100 = Very Good\n",
        "\n",
        "#@markdown T4 = Good\n",
        "\n",
        "#@markdown K80 = Meh\n",
        "\n",
        "#@markdown P4 = Aight\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-90efad9a-62e1-ff15-6eb1-6f49400ede41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dViyQpDNhgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "outputId": "a851595e-7677-467b-debf-720041e9601b"
      },
      "source": [
        "#@markdown #**Anti-Disconnect for Google Colab**\n",
        "#@markdown ## Run this to stop it from disconnecting automatically \n",
        "#@markdown  **(It will anyhow disconnect after 6 - 12 hrs for using the free version of Colab.)**\n",
        "#@markdown  *(Colab Pro users will get about 24 hrs usage time)*\n",
        "#@markdown ---\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0RC4VN-8RME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "1db9d69c-eb8c-4e61-fce7-1f8efcf4bf32"
      },
      "source": [
        "#@title Connect Google Drive and create folders\n",
        "#@markdown root_path will be created in `/content/drive/MyDrive/AI/VQGAN_Video`\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = \"test\" #@param {type: \"string\"}\n",
        "abs_root_path = f'/content/drive/MyDrive/AI/VQGAN_Video/{root_path}'\n",
        "\n",
        "from pathlib import Path\n",
        "def checkMakePath(filepath):\n",
        "    make_file = Path(filepath)\n",
        "    if not make_file.exists():\n",
        "      !mkdir --parent {make_file}\n",
        "      print(f'Made {filepath}')\n",
        "    else:\n",
        "      print(f'filepath {filepath} exists.')\n",
        "\n",
        "inDirPath = f'{abs_root_path}/in'\n",
        "checkMakePath(inDirPath)\n",
        "outDirPath = f'{abs_root_path}/out'\n",
        "checkMakePath(outDirPath)\n",
        "esrOutDirPath = f'{abs_root_path}/esrOut'\n",
        "checkMakePath(esrOutDirPath)\n",
        "rendersDirPath = f'{abs_root_path}/renders'\n",
        "checkMakePath(rendersDirPath)\n",
        "videoInPath = f'{abs_root_path}/VideoIn'\n",
        "checkMakePath(videoInPath)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Made /content/drive/MyDrive/AI/VQGAN_Video/test/in\n",
            "Made /content/drive/MyDrive/AI/VQGAN_Video/test/out\n",
            "Made /content/drive/MyDrive/AI/VQGAN_Video/test/esrOut\n",
            "Made /content/drive/MyDrive/AI/VQGAN_Video/test/renders\n",
            "Made /content/drive/MyDrive/AI/VQGAN_Video/test/VideoIn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSfISAhyPmyp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "664b9c19-b6ef-4a88-efb9-800d2156ea7a"
      },
      "source": [
        "#@markdown #**Installation of libraries**\n",
        "# @markdown This cell will take a little while because it has to download several libraries\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!pip install kornia\n",
        "!pip install einops"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 164 (delta 33), reused 55 (delta 29), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (164/164), 8.91 MiB | 16.90 MiB/s, done.\n",
            "Resolving deltas: 100% (73/73), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1061, done.\u001b[K\n",
            "remote: Counting objects: 100% (251/251), done.\u001b[K\n",
            "remote: Compressing objects: 100% (243/243), done.\u001b[K\n",
            "remote: Total 1061 (delta 12), reused 229 (delta 7), pack-reused 810\u001b[K\n",
            "Receiving objects: 100% (1061/1061), 350.33 MiB | 33.19 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
            "\u001b[K     |████████████████████████████████| 74 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
            "\u001b[K     |████████████████████████████████| 925 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting PyYAML>=5.1.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.9 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 77.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Collecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 67.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu111)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 62.8 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.41.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
            "Building wheels for collected packages: ftfy, antlr4-python3-runtime, future\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=8e11ac18281e40ab0360bddece6069f1d5d6752345f5337431e329f7b301c1f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=43936fb0164900186a9e6ac730ea0827a2ec96d175df8924fadd2461dd40032d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=5ff4efa19a6330a8aad389c77eef78fffe9ddaa751f95b5332e026b50e40dfa1\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built ftfy antlr4-python3-runtime future\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, antlr4-python3-runtime, pytorch-lightning, omegaconf, ftfy\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.7.4.post0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 fsspec-2021.10.1 ftfy-6.0.3 future-0.18.2 multidict-5.2.0 omegaconf-2.1.1 pyDeprecate-0.3.1 pytorch-lightning-1.4.9 torchmetrics-0.5.1 yarl-1.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.5.11-py2.py3-none-any.whl (336 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 71 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 102 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 112 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 122 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 133 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 143 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 153 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 163 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 174 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 184 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 194 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 204 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 215 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 225 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 235 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 245 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 256 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 266 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 276 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 286 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 296 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 307 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 317 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 327 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 336 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.9.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->kornia) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (2.4.7)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.5.11\n",
            "Collecting einops\n",
            "  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPKRgKREkQUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "3f5616e9-9611-4d4c-8ca5-8f0e18c1f2a2"
      },
      "source": [
        "#@title Selection of models to download\n",
        "#@markdown By default, the notebook downloads Model 16384 from ImageNet. There are others such as ImageNet 1024, COCO-Stuff, WikiArt 1024, WikiArt 16384, FacesHQ or S-FLCKR, which are not downloaded by default, since it would be in vain if you are not going to use them, so if you want to use them, simply select the models to download.\n",
        "\n",
        "imagenet_1024 = False #@param {type:\"boolean\"}\n",
        "imagenet_16384 = False #@param {type:\"boolean\"}\n",
        "coco = False #@param {type:\"boolean\"}\n",
        "faceshq = False #@param {type:\"boolean\"}\n",
        "wikiart_16384 = False #@param {type:\"boolean\"}\n",
        "sflckr = False #@param {type:\"boolean\"}\n",
        "\n",
        "if imagenet_1024:\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n",
        "if imagenet_16384:\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n",
        "if coco:\n",
        "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
        "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
        "if faceshq:\n",
        "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
        "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
        "if wikiart_16384:\n",
        "  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml' #WikiArt 16384\n",
        "  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt' #WikiArt 16384\n",
        "if sflckr:\n",
        "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
        "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   692  100   692    0     0    789      0 --:--:-- --:--:-- --:--:--   789\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  934M  100  934M    0     0  14.7M      0  0:01:03  0:01:03 --:--:-- 14.9M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXMSuW2EQWsd",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Loading libraries and definitions**\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "import kornia.augmentation as K\n",
        "\n",
        "def noise_gen(shape):\n",
        "    n, c, h, w = shape\n",
        "    noise = torch.zeros([n, c, 1, 1])\n",
        "    for i in reversed(range(5)):\n",
        "        h_cur, w_cur = h // 2**i, w // 2**i\n",
        "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n",
        "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n",
        "    return noise\n",
        "\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "# def replace_grad(fake, real):\n",
        "#     return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        "\n",
        "\n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        "\n",
        "replace_grad = ReplaceGrad.apply\n",
        "\n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        "# clamp_with_grad = torch.clamp\n",
        "\n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n",
        "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n",
        "\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n",
        "    input_normed = F.normalize(input, dim=-1)\n",
        "    target_normed = F.normalize(target, dim=-1)\n",
        "    logits = input_normed @ target_normed.T * logit_scale\n",
        "    if labels is None:\n",
        "        labels = torch.arange(len(input), device=logits.device)\n",
        "    return F.cross_entropy(logits, labels)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n",
        "\n",
        "    def set_cut_pow(self, cut_pow):\n",
        "      self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        cutouts_full = []\n",
        "        \n",
        "        min_size_width = min(sideX, sideY)\n",
        "        lower_bound = float(self.cut_size/min_size_width)\n",
        "        \n",
        "        for ii in range(self.cutn):\n",
        "            \n",
        "            \n",
        "          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "          size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n",
        "          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n",
        "\n",
        "          offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "          offsety = torch.randint(0, sideY - size + 1, ())\n",
        "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "\n",
        "        \n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "\n",
        "        # if args.use_augs:\n",
        "        #   cutouts = augs(cutouts)\n",
        "\n",
        "        # if args.noise_fac:\n",
        "        #   facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "        #   cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "        \n",
        "\n",
        "        return clamp_with_grad(cutouts, 0, 1)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n",
        "        model = vqgan.GumbelVQ(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def forward(self, input):\n",
        "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "        diff = x_diff**2 + y_diff**2 + 1e-8\n",
        "        return diff.mean(dim=1).sqrt().mean()\n",
        "\n",
        "class GaussianBlur2d(nn.Module):\n",
        "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.value = value\n",
        "        if not window:\n",
        "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n",
        "        if sigma:\n",
        "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n",
        "            kernel /= kernel.sum()\n",
        "        else:\n",
        "            kernel = torch.ones([1])\n",
        "        self.register_buffer('kernel', kernel)\n",
        "\n",
        "    def forward(self, input):\n",
        "        n, c, h, w = input.shape\n",
        "        input = input.view([n * c, 1, h, w])\n",
        "        start_pad = (self.kernel.shape[0] - 1) // 2\n",
        "        end_pad = self.kernel.shape[0] // 2\n",
        "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n",
        "        input = F.conv2d(input, self.kernel[None, None, None, :])\n",
        "        input = F.conv2d(input, self.kernel[None, None, :, None])\n",
        "        return input.view([n, c, h, w])\n",
        "\n",
        "class EMATensor(nn.Module):\n",
        "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n",
        "    def __init__(self, tensor, decay):\n",
        "        super().__init__()\n",
        "        self.tensor = nn.Parameter(tensor)\n",
        "        self.register_buffer('biased', torch.zeros_like(tensor))\n",
        "        self.register_buffer('average', torch.zeros_like(tensor))\n",
        "        self.decay = decay\n",
        "        self.register_buffer('accum', torch.tensor(1.))\n",
        "        self.update()\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def update(self):\n",
        "        if not self.training:\n",
        "            raise RuntimeError('update() should only be called during training')\n",
        "\n",
        "        self.accum *= self.decay\n",
        "        self.biased.mul_(self.decay)\n",
        "        self.biased.add_((1 - self.decay) * self.tensor)\n",
        "        self.average.copy_(self.biased)\n",
        "        self.average.div_(1 - self.accum)\n",
        "\n",
        "    def forward(self):\n",
        "        if self.training:\n",
        "            return self.tensor\n",
        "        return self.average\n",
        "\n",
        "%mkdir /content/vids"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KeAxAGEfBZL7"
      },
      "source": [
        "#@markdown #**Split video into images - FFmpeg**\n",
        "#@markdown Upload your video to the folder `{projectDir}/VideoIn` and enter the name below and run the cell.\n",
        "\n",
        "#@markdown  You can skip this step by uploading your png images to `root_path/in` (specified above). Name each frame in-0000.png in ascending order. \n",
        "\n",
        "\n",
        "from subprocess import Popen, PIPE\n",
        "input_video='input.mp4' #@param{type:\"string\"}\n",
        "\n",
        "p = Popen(['ffmpeg', '-y', '-i', f'{videoInPath}/{input_video}', '-qscale:', '2', f'{inDirPath}/in-%04d.png'], stdin=PIPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4OtaLbHBN6"
      },
      "source": [
        "## **Arguments**         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLw9p5Rzacso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "687375e9-e918-40a8-ede3-570f7ab91e32"
      },
      "source": [
        "#@markdown #**Settings and inputs**\n",
        "#@markdown Double-click here for more indepth settings\n",
        "#@markdown First time you select a new VQGAN Model, this cell will download it.\n",
        "#@markdown  \n",
        "#@markdown  .\n",
        "\n",
        "prompts=[\"a beautiful realistic painting by Cedric Peyravernay of a summoning ritual, with a lot of wide textured brush strokes, trending on artstation,\"]#@param{type:\"raw\"}\n",
        "size=[800, 550] #@param{type:\"raw\"}\n",
        "# init_image= '/content/wMsuAbPmE8SMgAAAABJRU5ErkJggg.png' #@param{type:\"string\"}\n",
        "init_weight= 1 #@param{type:\"raw\"}\n",
        "step_size = 0.04 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "\n",
        "model_id = \"wikiart_16384\" #@param [\"vqgan_imagenet_f16_16384\", \"vqgan_imagenet_f16_1024\", \"wikiart_1024\", \"wikiart_16384\", \"coco\", \"faceshq\", \"sflckr\"]\n",
        "\n",
        "\n",
        "# cutouts / crops\n",
        "cutn=64 #@param{type:\"raw\"}\n",
        "cut_pow=1 #@param{type:\"raw\"}\n",
        "\n",
        "display_freq=25 #@param{type:\"raw\"}\n",
        "seed=12343425 #@param{type:\"raw\"}\n",
        "use_augs = True #@param{type:\"boolean\"}\n",
        "\n",
        "\n",
        "model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n",
        "          \"wikiart_1024\":\"WikiArt 1024\", \"wikiart_16384\":\"WikiArt 16384\", \"coco\":\"COCO-Stuff\", \n",
        "          \"drive/MyDrive/colab/coco\":\"COCO-Stuff (Local)\",\"faceshq\":\"FacesHQ\", \"sflckr\":\"S-FLCKR\"}\n",
        "model_name = model_names[model_id]  \n",
        "\n",
        "if model_id == vqgan_imagenet_f16_1024:\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\n",
        "if model_id == vqgan_imagenet_f16_16384:\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 16384\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' #ImageNet 16384\n",
        "if model_id == coco:\n",
        "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
        "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
        "if model_id == faceshq:\n",
        "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
        "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
        "if model_id == wikiart_16384:\n",
        "  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml' #WikiArt 16384\n",
        "  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt' #WikiArt 16384\n",
        "if model_id == sflckr:\n",
        "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
        "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    \n",
        "    prompts=prompts,\n",
        "    size=size, \n",
        "    init_image= None,\n",
        "    init_weight= 0,\n",
        "\n",
        "    # clip model settings\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config=f'{model_id}.yaml',\n",
        "    vqgan_checkpoint=f'{model_id}.ckpt',\n",
        "    step_size=step_size,\n",
        "    \n",
        "    # cutouts / crops\n",
        "    cutn=64,\n",
        "    cut_pow=1,\n",
        "\n",
        "    # display\n",
        "    display_freq= display_freq,\n",
        "    seed= seed,\n",
        "    use_augs= use_augs,\n",
        "    noise_fac= 0.1,\n",
        "    ema_val = 0.99,\n",
        "\n",
        "    record_generation=True,\n",
        "\n",
        "    # noise and other constraints\n",
        "    use_noise = None,\n",
        "    constraint_regions = False,#\n",
        "    \n",
        "    \n",
        "    # add noise to embedding\n",
        "    noise_prompt_weights = None,\n",
        "    noise_prompt_seeds = [14575],#\n",
        "\n",
        "    # mse settings\n",
        "    mse_withzeros = True,\n",
        "    mse_decay_rate = 50,\n",
        "    mse_epoches = 5,\n",
        "    mse_quantize = True,\n",
        "\n",
        "    # end itteration\n",
        "    max_itter = -1,\n",
        ")\n",
        "\n",
        "mse_decay = 0\n",
        "if args.init_weight:\n",
        "  mse_decay = args.init_weight / args.mse_epoches\n",
        "\n",
        "# <AUGMENTATIONS>\n",
        "augs = nn.Sequential(\n",
        "    \n",
        "    K.RandomHorizontalFlip(p=0.5),\n",
        "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n",
        "    K.RandomPerspective(0.2,p=0.4, ),\n",
        "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n",
        "\n",
        "    )\n",
        "\n",
        "noise = noise_gen([1, 3, args.size[0], args.size[1]])\n",
        "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n",
        "image.save('init3.png')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2463d6912c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvqgan_imagenet_f16_1024\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' #ImageNet 1024\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1'  #ImageNet 1024\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vqgan_imagenet_f16_1024' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crRQdV3jPSvw"
      },
      "source": [
        "# **Constraints**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSUp-6M0m-Dz",
        "cellView": "form"
      },
      "source": [
        "#@markdown #*Double-click here and edit me if you like*\n",
        "#@markdown ---\n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "if args.constraint_regions and args.init_image:\n",
        "  \n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "  toksX, toksY = args.size[0] // 16, args.size[1] // 16\n",
        "\n",
        "  pil_image = Image.open(args.init_image).convert('RGB')\n",
        "  pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "\n",
        "  width, height = pil_image.size\n",
        "\n",
        "  d = ImageDraw.Draw(pil_image)\n",
        "  for i in range(0,width,16):\n",
        "      d.text((i+4,0), f\"{int(i/16)}\", fill=(50,200,100))\n",
        "  for i in range(0,height,16):\n",
        "      d.text((4,i), f\"{int(i/16)}\", fill=(50,200,100))\n",
        "\n",
        "  pil_image = TF.to_tensor(pil_image)\n",
        "\n",
        "  print(pil_image.shape)\n",
        "  for i in range(pil_image.shape[1]):\n",
        "    for j in range(pil_image.shape[2]):\n",
        "      if i%16 == 0 or j%16 ==0:\n",
        "        pil_image[:,i,j] = 0\n",
        "\n",
        "  # select region\n",
        "  c_h = [16,32]\n",
        "  c_w = [0,40]\n",
        "\n",
        "  c_hf = [i*16 for i in c_h]\n",
        "  c_wf = [i*16 for i in c_w]\n",
        "\n",
        "  pil_image[0,c_hf[0]:c_hf[1],c_wf[0]:c_wf[1]] = 0\n",
        "\n",
        "  TF.to_pil_image(pil_image.cpu()).save('progress.png')\n",
        "  display.display(display.Image('progress.png'))\n",
        "\n",
        "  z_mask = torch.zeros([1, 256, int(height/16), int(width/16)]).to(device)\n",
        "  z_mask[:,:,c_h[0]:c_h[1],c_w[0]:c_w[1]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmuK7MTroHHE"
      },
      "source": [
        "#**Final Steps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7EDme5RYCrt",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Fire up the AI**\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print('Using device:', device)\n",
        "print('using prompts: ', args.prompts)\n",
        "\n",
        "tv_loss = TVLoss() \n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "mse_weight = args.init_weight\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "# e_dim = model.quantize.e_dim\n",
        "\n",
        "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "    e_dim = 256\n",
        "    n_toks = model.quantize.n_embed\n",
        "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n",
        "else:\n",
        "    e_dim = model.quantize.e_dim\n",
        "    n_toks = model.quantize.n_e\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "\n",
        "f = 2**(model.decoder.num_resolutions - 1)\n",
        "toksX, toksY = args.size[0] // f, args.size[1] // f\n",
        "\n",
        "if args.seed is not None:\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "    pil_image = TF.to_tensor(pil_image)\n",
        "    if args.use_noise:\n",
        "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n",
        "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "else:\n",
        "    \n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "\n",
        "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z = one_hot @ model.quantize.embed.weight\n",
        "    else:\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "z = EMATensor(z, args.ema_val)\n",
        "\n",
        "if args.mse_withzeros and not args.init_image:\n",
        "  z_orig = torch.zeros_like(z.tensor)\n",
        "else:\n",
        "  z_orig = z.tensor.clone()\n",
        "\n",
        "\n",
        "opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "if args.noise_prompt_weights and args.noise_prompt_seeds:\n",
        "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "    pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "    # pMs[0].embed = pMs[0].embed + Prompt(embed, weight, stop).embed.to(device)\n",
        "\n",
        "\n",
        "def synth(z, quantize=True):\n",
        "    if args.constraint_regions:\n",
        "      z = replace_grad(z, z * z_mask)\n",
        "\n",
        "    if quantize:\n",
        "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n",
        "      else:\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "\n",
        "    else:\n",
        "      z_q = z.model\n",
        "\n",
        "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "    out = synth(z.average, True)\n",
        "\n",
        "    TF.to_pil_image(out[0].cpu()).save('progress.png')   \n",
        "    display.display(display.Image('progress.png')) \n",
        "\n",
        "\n",
        "def ascend_txt():\n",
        "    global mse_weight\n",
        "\n",
        "    out = synth(z.tensor)\n",
        "\n",
        "    if args.record_generation:\n",
        "      with torch.no_grad():\n",
        "        global vid_index\n",
        "        out_a = synth(z.average, True)\n",
        "        TF.to_pil_image(out_a[0].cpu()).save(f'/content/vids/{vid_index}.png')\n",
        "        vid_index += 1\n",
        "\n",
        "    cutouts = make_cutouts(out)\n",
        "\n",
        "    if args.use_augs:\n",
        "      cutouts = augs(cutouts)\n",
        "\n",
        "    if args.noise_fac:\n",
        "      facs = cutouts.new_empty([args.cutn, 1, 1, 1]).uniform_(0, args.noise_fac)\n",
        "      cutouts = cutouts + facs * torch.randn_like(cutouts)\n",
        "\n",
        "    iii = perceptor.encode_image(normalize(cutouts)).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        \n",
        "        global z_orig\n",
        "        \n",
        "        result.append(F.mse_loss(z.tensor, z_orig) * mse_weight / 2)\n",
        "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n",
        "\n",
        "            if args.mse_quantize:\n",
        "              z_orig = vector_quantize(z.average.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n",
        "            else:\n",
        "              z_orig = z.average.clone()\n",
        "\n",
        "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n",
        "              mse_weight = mse_weight - mse_decay\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "            else:\n",
        "              mse_weight = 0\n",
        "              print(f\"updated mse weight: {mse_weight}\")\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "\n",
        "vid_index = 0\n",
        "def train(i):\n",
        "    \n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    \n",
        "    loss = sum(lossAll)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    z.update()\n",
        "\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while True and i != args.max_itter:\n",
        "\n",
        "            train(i)\n",
        "\n",
        "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n",
        "              z = EMATensor(z.average, args.ema_val)\n",
        "              opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n",
        "\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDUaCaRnUKMZ"
      },
      "source": [
        "# **Generate video**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT3hKb5gJUPq",
        "cellView": "form"
      },
      "source": [
        "#@markdown #*Double-click here and edit me*\n",
        "\n",
        "%cd vids\n",
        "\n",
        "images = \"%d.png\"\n",
        "video = \"/content/old_man_iceberg.mp4\"\n",
        "!ffmpeg -r 30 -i $images -crf 20 -s 640x512 -pix_fmt yuv420p $video\n",
        "\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiZMW3kAUD1f"
      },
      "source": [
        "**Delete all frames from folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OXMWkXo7okMA"
      },
      "source": [
        "#@markdown Run this tab if you wanna clear all the genarated frames images\n",
        "\n",
        "\n",
        "%cd vids\n",
        "%rm *.png\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}